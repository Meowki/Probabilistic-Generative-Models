\chapter{Approximate Inference}

\textbf{NB:} This is based on \cite{andrieu2003introduction}, \cite{blei2017variational}, and Chapter 10 of \cite{bishop2006prml}.

\section{Motivation: intractable posteriors}

\paragraph{Bayesian inference in generative models.} A probabilistic generative model specifies a joint distribution over observed variables
\(x \in \mathcal X\) and latent variables \(z \in \mathcal Z\):
\[
p_\theta(x,z) = p_\theta(z)\,p_\theta(x \mid z),
\]
where \(\theta\) denotes model parameters.

Given observations \(x\), Bayesian inference aims to compute the posterior distribution
\[
p_\theta(z \mid x) = \frac{p_\theta(x,z)}{p_\theta(x)},
\qquad
p_\theta(x) = \int p_\theta(x,z)\,dz.
\]

The posterior encodes all the uncertainty related to the latent variable \(z\) after observing the data, and is the central object of interest in Bayesian modelling.

\begin{remark}
    From now on, we will drop the explicit dependency on the parameter $\theta$. From a Bayesian standpoint, we will consider that the latent variable $z$ encapsulates all (random) unknowns. This includes global variables such as parameters, and local variables such as cluster assignments. 
\end{remark}

\begin{example}[Hierarchical Gaussian mixture model]
\label{ex:hierarchical_GMM}
To illustrate the role of both global and local latent
variables in a PGM, we consider the following hierarchical GMM.

Let \(x_n \in \mathbb R^d\), \(n=1,\dots,N\), denote the observed data.
The latent variables are:
\begin{itemize}
  \item mixture weights \(\pi = (\pi_1,\dots,\pi_K)\in \Delta^{K-1}, \quad
\Delta^{K-1}
=
\left\{
\pi \in \mathbb{R}^K
:
\pi_k \ge 0 \ \forall k,\;
\sum_{k=1}^K \pi_k = 1
\right\}.,\)
  \item cluster means \(\mu_1,\dots,\mu_K \in \mathbb R^d\),
  \item cluster assignments \(z_1,\ldots, z_N \in \{1,\dots,K\}\).
\end{itemize}

The generative process is:
\begin{alignat*}{3}
\pi      &\sim \mathrm{Dirichlet}(\alpha) \\
\mu_k    &\sim \mathcal N(m_0, \Sigma_0) 
        &\qquad& k=1,\dots,K \\
z_n \mid \pi 
        &\sim \mathrm{Categorical}(\pi) 
        &\qquad& n=1,\dots,N \\
x_n \mid z_n, \{\mu_k\} 
        &\sim \mathcal N(\mu_{z_n}, \Sigma) 
        &\qquad& n=1,\dots,N.
\end{alignat*}

The resulting joint distribution factorises as
\[
p(x,z,\mu,\pi)
=
p(\pi)
\prod_{k=1}^K p(\mu_k)
\prod_{n=1}^N p(z_n \mid \pi)\,p(x_n \mid z_n,\mu).
\]

Given observations \(x_{1:N}\), the posterior given by 
\[
p(z_{1:N}, \mu_{1:K}, \pi \mid x_{1:N}),
\]
is analytically intractable due to the hierarchical coupling between the
latent variables in the the marginal 
\[
p( x_{1:N}) = \int p(z_{1:N}, \mu_{1:K}, \pi, x_{1:N})\d z_{1:N} \d \mu_{1:K} \d\pi.
\]

Fig.~\ref{fig:hierarchical_GMM} shows data generated by this model using hierarchical sampling ($N=400, K=4$), and Fig.~\ref{fig:hierarchical_gmm_tikz} shows the graphical model representation.


\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{img/week3_hierarchical_GMM.pdf}
    \caption{Samples from a hierarchical GMM: 2 dimensions, $N=400$ samples.}
    \label{fig:hierarchical_GMM}
\end{figure}

\begin{figure}[t]
\centering
\begin{tikzpicture}

% Nodes
\node[latent] (pi) {$\pi$};
\node[latent, right=1.6cm of pi] (z) {$z_n$};
\node[obs, right=2.4cm of z] (x) {$x_n$};
\node[latent, right=2.4cm of x] (mu) {$\mu_k$};


% Edges
\edge {pi} {z};
\edge {z} {x};
\edge {mu} {x};

% Plates
\plate {plateK} {(mu)} {$k=1,\dots,K$};
\plate {plateN} {(z)(x)} {$n=1,\dots,N$};

\end{tikzpicture}
\caption{Graphical model of a hierarchical Gaussian mixture model.
The global mixture weights $\pi$ and component means $\{\mu_k\}_{k=1}^K$
govern the generation of latent cluster assignments $z_n$ and observations
$x_n$.}
\label{fig:hierarchical_gmm_tikz}
\end{figure}


\end{example}


\begin{example}[Bayesian linear regression]
Let \(\{(x_n,y_n)\}_{n=1}^N\) be the observed data, where \(\{x_n\}_{n=1}^N \subset \mathbb R^d\) are inputs and \(\{y_n\}_{n=1}^N \subset \mathbb R\) are outputs. The latent variable is the regression weight vector \(w \in \mathbb R^d\), which is the parameter of the model. Notice that in this case the parameter is the only global latent variable and there are no local latent variables. 

The generative model is defined by
\[
w \sim \mathcal N(m_0, \Sigma_0),
\qquad
y_n \mid w, x_n \sim \mathcal N(x_n^\top w, \sigma^2),
\quad n=1,\dots,N.
\]

Denoting the input matrix \(X \in \mathbb R^{N\times d}\) with rows
\(x_n^\top\), and the observation vector \(y=(y_1,\dots,y_N)^\top\), the likelihood can be written compactly as
\[
y \mid w, X \sim \mathcal N(Xw, \sigma^2 I_N),
\]
where $I_N$ denotes the $N\times N$ identity matrix. Since the joint distribution factorises as
\(
p(y,w\mid X)
=
p(w)\prod_{n=1}^N p(y_n\mid w,x_n)
\), the posterior distribution over the latent weights can be expressed as
\[
p(w\mid X,y) \propto p(w)\prod_{n=1}^N p(y_n\mid w,x_n),
\]
which admits the closed-form Gaussian solution
\[
p(w\mid X,y)=\mathcal N(m_N,\Sigma_N),
\]
with 
\[
\Sigma_N^{-1}=\Sigma_0^{-1}+\frac{1}{\sigma^2}X^\top X,
\quad
m_N=\Sigma_N\!\left(\Sigma_0^{-1}m_0+\frac{1}{\sigma^2}X^\top y\right).
\]

This model admits exact Bayesian inference is tractable. Fig.~\ref{fig:bayesian_LM} shows samples from this model alongside the linear function corresponding to the latent weight. Fig.~\ref{fig:bayesian_linear_regression_gm} shows the graphical model representation.

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{img/week3_bayesian_linear_reg.pdf}
    \caption{Samples from a Bayesian linear model: 1 dimension, $N=400$ samples.}
    \label{fig:bayesian_LM}
\end{figure}

\begin{figure}[t]
\centering
\begin{tikzpicture}

% Nodes

\node[const] (hyp) {$m_0,\Sigma_0$};
\node[latent, right = 1.6cm of hyp] (w) {$w$};
\node[obs, right=1.6cm of w] (y) {$y_n$};
\node[const, right=2.2cm of y] (x) {$x_n$};



% Edges
\edge {w} {y};
\edge {x} {y};
\edge {hyp} {w};

% Plate
\plate {plateN} {(y)(x)} {$n=1,\dots,N$};

\end{tikzpicture}
\caption{Graphical model of Bayesian linear regression. The global weight vector $w$ generates observations $y_n$ given input $x_n$ for $n=1,\dots,N$.}
\label{fig:bayesian_linear_regression_gm}
\end{figure}



\end{example}


\begin{example}[State space model]
Let us now consider a latent variable model describing the evolution of a dynamical
system observed through noisy measurements.

Let \(z_t \in \mathbb R^m\), \(t=1,\dots,T\), denote the latent state at time \(t\),
and let \(x_t \in \mathbb R^d\) denote the corresponding observation.
The latent states and observations are linked through a transition model and an
observation model.

The generative process is defined as follows:
\begin{alignat*}{3}
z_1 &\sim p(z_1) \\
z_t & =   f(z_{t-1}) + \epsilon_t,
      &\qquad& t=2,\dots,T \\
x_t & =   h(z_t) + \eta_t,
      &\qquad& t=1,\dots,T,
\end{alignat*}

where \(f:\R^m \to \R^m\) is the (possibly nonlinear) state transition
function, \(h:\R^m \to \R^d\) is the observation function, and  $\epsilon_t\sim p_\text{transition}$ and $\eta_t\sim p_\text{observation}$ are the process and observation noise sources respectively.

The joint distribution over latent states and observations factorises as
\[
p(x_{1:T}, z_{1:T})
=
p(z_1)
\prod_{t=2}^T p(z_t \mid z_{t-1})
\prod_{t=1}^T p(x_t \mid z_t).
\]

Given the observations \(x_{1:T}\), the posterior
\(
p(z_{1:T} \mid x_{1:T})
\)
is tractable only when
\(f\) and \(h\) are linear, and both $p_\text{transition}$ and $p_\text{observation}$ are Gaussian, otherwise, approximate inference methods are required. Fig.~\ref{fig:SSM} shows a sample form this model, and Fig.~\ref{fig:ssm_graphical_model} the graphical model representation.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{img/week3_SSM.pdf}
    \caption{Samples from a nonlinear state space model: 1 dimension, $N=400$ samples.}
    \label{fig:SSM}
\end{figure}

\begin{figure}[t]
\centering
\begin{tikzpicture}

% Nodes (generic time index t)
\node[latent] (zprev) {$z_{t-1}$};
\node[latent, right=1.4cm of zprev] (z) {$z_t$};
\node[obs, right=1.4cm of z] (x) {$x_t$};
\node[latent, above=1.2cm of z] (eps) {$\epsilon_t$};
\node[latent, above=1.2cm of x] (eta) {$\eta_t$};

% Edges
\edge {zprev} {z};
\edge {z} {x};

% Plate over time
\plate {plateT} {(zprev)(z)(x)} {$t=2,\dots,T$};

% Initial state (t=1)
\node[latent, left=1.4cm of zprev] (z1) {$z_1$};
\node[obs, left=1.4cm of z1] (x1) {$x_1$};
\edge {z1} {x1};
\edge {z1} {zprev};
\edge {eps} {z};
\edge {eta} {x};


% Plate for observations at t=1 (optional clarity)
%\plate {plate1} {(z1)(x1)} {$t=1$};

\end{tikzpicture}
\caption{Graphical model of a (possibly nonlinear) state space model. Latent states
$\{z_t\}_{t=1}^T$ form a Markov chain via the transition dynamics, and each observation
$x_t$ depends on the corresponding state $z_t$ through the observation model.}
\label{fig:ssm_graphical_model}
\end{figure}


\end{example}



Approximate inference replaces the true posterior $p(z \mid x)$ with an approximation, say $q(z$), that is as close a possible to the true posterior, while at the same time allows for computing expectations and scalable computation. In this sense, there are two dominant paradigms for computing $q$. 

\begin{itemize}
    \item Monte Carlo methods, which approximate the posterior via samples $z^{(s)} \sim p(z\mid x)$. This estimate is asymptotically exact but often computationally expensive.
    \item Variational methods, which provide a functional approximation of the posterior by solving an optimization problem:
    \[
q^\star(z)
=
\arg\min_{q \in \mathcal Q}
\mathrm{KL}\big(q(z)\,\|\,p(z\mid x)\big),
\]
where \(\mathcal Q\) is a tractable family of distributions. Variational approximations are fast and scalable, but introduce bias.
\end{itemize}


\section{Markov chain Monte Carlo}

In Bayesian inference, the posterior is usually used to computing expectations of the form
\[
\mathbb E_{p(z\mid x)}[f(z)] = \int f(z)\,p(z\mid x)\,dz.
\]

When direct evaluation of this integral is infeasible, an alternative is to use a Monte Carlo approximation, that is, 
\[
\mathbb E_{p(z\mid x)}[f(z)]
\;\approx\;
\frac{1}{S}\sum_{s=1}^S f(z^{(s)}),
\qquad
z^{(s)} \sim p(z\mid x).
\]

The law of large numbers guarantees that this estimator converges almost surely as \(S \to \infty\). Furthermore, the root-mean-square error of this estimator decreases at a rate $1/\sqrt{S}$

However, the challenge in Bayesian inference is to obtain the samples $\{z^{(s)}\}_{s=1}^S$. This is because the posterior
\[
p(z\mid x) = \frac{p(x,z)}{p(x)},
\]
is rarely tractable and expensive to compute numerically in high dimensions due to the integral form of the normalizing constant  
\[
p(x) = \int p(x,z)\,dz = \int p(x \mid z)p(z)\,dz.
\]
As a consequence, direct sampling from \(p(z\mid x)\) is not possible.

The rationale behind Markov Chain Monte Carlo (MCMC) methods is to construct a Markov chain whose limiting stationary distribution is the desired posterior. Recall that a Markov chain \(\{z^{(t)}\}_{t \in \N}\) is defined by a transition kernel $T(z' \mid z)$,
where
\[
\mathbb P(z^{(t+1)} = z' \mid z^{(t)} = z) = T(z' \mid z).
\]

For this Markov chain to have the posterior $p(z \mid x)$ as its limiting distribution, two conditions need to hold. First, the posterior has to be the chain's stationary distribution, that is, 
\[
\int p(z\mid x)\,T(z' \mid z)\,dz = p(z'\mid x).
\]
This means that if the chain starts at $z \sim p(z \mid x)$, then after one step the chain remains in the same distribution. Second, the chain should converge to its stationary distribution.  A sufficient (but not necessary) condition for stationarity is the
\emph{detailed balance} condition,
\begin{equation}
    p(z\mid x)\,T(z' \mid z)
=
p(z'\mid x)\,T(z \mid z'),
\label{eq:detailed_balance}    
\end{equation}
which implies that \(p(z\mid x)\) is a stationary distribution of the Markov
chain. The detailed balance condition states that, under the stationary distribution, the probability flow from state \(z\) to state \(z'\) is exactly balanced by the flow from \(z'\) to \(z\). As a result, there is no net movement of probability mass, and the target distribution remains invariant under the Markov chain dynamics.

To ensure convergence to the stationary distribution from an arbitrary initial
state, additional regularity conditions are required. In particular, the Markov
chain should be \emph{irreducible}, meaning that it is possible to reach any
state from any other state with positive probability, and \emph{aperiodic},
meaning that the chain does not get trapped in deterministic cycles. Under these conditions, the distribution of \(z^{(t)}\) converges to
\(p(z\mid x)\) as \(t \to \infty\).


\paragraph{Metropolis--Hastings.} A construction of a transition kernel satisfying the convergence conditions above can be obtained in two steps. First, a candidate state \(z'\) is drawn from an arbitrary proposal distribution \(\pi(z' \mid z)\). Second, this proposal is accepted or rejected according to a criterion that depends on how likely \(z'\) is under the target distribution \(p(z \mid x)\).


Given the current state \(z\), the Metropolis--Hastings (MH) update proceeds as follows:
\begin{enumerate}
  \item Propose a new state
  \[
  z' \sim \pi(z' \mid z).
  \]
  \item Compute the probability of acceptance
  \begin{equation}
      \alpha(z,z')
  =
  \min\!\left(
  1,\;
  \frac{p(z'\mid x)\,\pi(z \mid z')}
       {p(z\mid x)\,\pi(z' \mid z)}
  \right).
    \label{eq:MH_acceptance}    
  \end{equation}
    \item Set the new sample as
  \[
  z^{(t+1)} =
  \begin{cases}
    z', & \text{with probability } \alpha(z,z'),\\
    z,  & \text{otherwise.}
  \end{cases}
  \]
\end{enumerate}


\begin{remark}
    MH does not require knowledge of the normalising constant \(p(x)\). As a consequence, since \(p(z\mid x) \propto p(x,z)\), the acceptance probability can be computed using the joint density \(p(x,z)\).
\end{remark}



\begin{remark}
    MH's transition kernel satisfies the detailed balance condition in eq.~\eqref{eq:detailed_balance}. Therefore, \(p(z\mid x)\) is a stationary limiting distribution of the chain built my MH.
\end{remark}

\begin{remark}
    A particular instance of MH can be identified by choosing a symmetric proposal, that is, $q(z \mid z') = q(z' \mid z)$. In this case, the acceptance probability reduces to 
      \[
    \alpha(z,z')
    =
    \min\!\left(
    1,\;
    \frac{p(z'\mid x)}
        {p(z\mid x)}
    \right).
    \]
    This is know as the Metropolis method. 
\end{remark}

In practice, a common choice for the proposal is simply a random walk
\[
\pi(z' \mid z) = \mathcal N(z, \sigma^2 I),
\]
however, though simple to implement, random-walk MH can mix poorly in high-dimensional or highly-correlated posteriors.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{img/week3_MH_GMM.pdf}
    \caption{Samples from a 2D 2-Gaussian mixture using Metropolis Hastings: $N=10,000$ samples. Observe the poor performance revealed by the number of accepted samples divided by the number of proposed samples.}
    \label{fig:MH_GMM}
\end{figure}

Fig.~\ref{fig:MH_GMM} shows an implementation of MH to sample from a mixture of 2 Gaussians using a single Gaussian as proposal.



\paragraph{Gibbs sampling.} 

To develop a MH variant that exploits conditional structure in the posterior distribution, the proposal over each coordinate of $z$ can be coupled to previously sampled coordinates. Let us denote
\(
z = (z_1, \dots, z_m),
\)
and update each component by sampling from its conditional distribution:
\[
z_i \sim p(z_i \mid z_{-i}, x),
\]
where \(z_{-i}\) denotes all components except \(z_i\).

A complete Gibbs update consists of sequentially sampling:
\[
z_1 \sim p(z_1 \mid z_2, \dots, z_m, x),
\]
\[
z_2 \sim p(z_2 \mid z_1, z_3, \dots, z_m, x),
\]
\[
\vdots
\]
\[
z_m \sim p(z_m \mid z_1, \dots, z_{m-1}, x).
\]
To see that Gibbs is a particular instance of MH, let us first denote the target posterior by  \(r(z) = p(z \mid x)\) for notational simplicity, and consider a fixed index \(i \in \{1,\dots,m\}\). To update the \(i\)-th coordinate while keeping \(z_{-i}\) fixed, Gibbs samples directly from the conditional posterior
\[
z_i' \sim r(z_i \mid z_{-i}),
\]
and defines the proposed sample
\[
z' = (z_i', z_{-i}).
\]
For the complete variable $z$, this  corresponds to a proposal distribution
\begin{align}
    \pi(z' \mid z)
    &= \pi(z'_i, z'_{-i} \mid z_i,z_{-i})\nonumber\\
     &= \pi(z'_i \mid z_i,z_{-i}, z'_{-i})\pi(z'_{-i} \mid z_i,z_{-i})\nonumber\\
     &= \pi(z'_i \mid z_{-i})\pi(z'_{-i} \mid z_{-i})\nonumber\\
    &= r(z_i' \mid z_{-i})\,\mathbf 1\{z'_{-i} = z_{-i}\}. \label{eq:Gibbs_proposal}
\end{align}

To compute the acceptance probability in eq.~\eqref{eq:MH_acceptance}, we can factorise

Using the factorization
\[
r(z) = r(z_{-i})\,r(z_i \mid z_{-i}),
\qquad
r(z') = r(z_{-i})\,r(z_i' \mid z_{-i}),
\]
together with the proposal in eq.~\eqref{eq:Gibbs_proposal} (recall that $z'_{-i} = z_{-i}$)
\[
\pi(z' \mid z) = r(z_i' \mid z_{-i}),
\qquad
\pi(z \mid z') = r(z_i \mid z_{-i}),
\]
we obtain
\[
\frac{r(z')\,\pi(z \mid z')}
     {r(z)\,\pi(z' \mid z)}
=
\frac{
r(z_{-i})\,r(z_i' \mid z_{-i})\,r(z_i \mid z_{-i})
}{
r(z_{-i})\,r(z_i \mid z_{-i})\,r(z_i' \mid z_{-i})
}
= 1.
\]

Therefore, the acceptance probability is
\[
\alpha(z,z') = 1,
\]
meaning that the proposed move is always accepted.


\begin{remark}
    Gibbs sampling is a special case of MH, in which the proposal distribution is the exact full conditional distribution, and the acceptance probability is identically equal to one. Furthermore, each update (not necessarily the full update) leaves the joint posterior invariant.
\end{remark}


\paragraph{Practical considerations.} There are relevant implementation practices to be taken into account when using MCMC. 

\textbf{Burn-in period:} Though MH is guaranteed to converge to its stationary distribution, the fact that the chain starts from an arbitrary distribution implies that the initial samples will not be representative of the target posterior. Therefore, the initial samples should be discarded as invalid samples from $p(z \mid x)$. When the samples are representative from the target, we say that the chain has \emph{mixed}. Furthermore, assessment of this convergence must be assessed empirically.  

\textbf{Autocorrelation:} The samples used for Monte Carlo integration should be i.i.d.~samples from the target (posterior) distributions; in fact, when correlated samples are used it is the \emph{effective sample size} that governs the quality of the approximation. Since the samples are generated by a chain with a correlated transition kernel, consecutive samples are not independent (in general correlated) by construction. To alleviate this, the samples from the chain should be \emph{thinned}, that is, to consider a subset of samples, for instance, one every 50 samples.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{img/week3_Gibbs_vs_MH.pdf}
    \caption{Samples from a 2D highly correlated Gaussian using Gibbs (right) and Metropolis Hastings (right): $N=1,000$ samples.}
    \label{fig:MH_vs_Gibbs}
\end{figure}

Fig.~\ref{fig:MH_vs_Gibbs} shows an implementation of Gibbs and MH sampling from a correlated Gaussian. Observe how with 1,000 samples Gibbs fully explores the support of the distribution while MH struggle to propose valid samples.


\section{The Laplace approximation}

Before focusing on variational inference in detail, let us study the following local approximation to the posterior. Consider the mode of the posterior (or one of the modes in the multimodal case), given by $\hat z = \arg\max_z \log p(z\mid x)$. Then, a second-order Taylor expansion of the log-posterior around this mode gives (recall that $\nabla \log p(z\mid x)|_{z=\hat{z}} =0 $):
\[
\log p(z\mid x) \approx \log p(\hat z\mid x) - \tfrac12 (z-\hat z)^\top H (z-\hat z),
\quad
H := -\nabla^2 \log p(z\mid x)\big|_{z=\hat z}.
\]
The equivalent approximating distribution can be calculated simply by
\[
q(z) = \exp\left(
{ \log p(\hat z\mid x)} - \tfrac12 (z-\hat z)^\top H (z-\hat z)
\right)
\propto
\exp\left(
 - \tfrac12 (z-\hat z)^\top H (z-\hat z)
\right).
\]
Therefore, the approximating distribution is a Gaussian centered in the mode, with a variance given by the curvature of the of the true posterior at the mode: 
Hence,
\[
q(z) = \cN\!\left(z \mid \hat z,\ H^{-1}\right).
\]
\begin{remark}
    This approximation only requires the maximiser $\hat z$ and the Hessian of $\log p(z\mid x) $, and not the actual value of the mode $\log p(\hat z \mid x)$. This is because the normalising constant can be calculated from the curvature when the distribution is Gaussian.  
\end{remark}

Fig.~\ref{fig:LaplaceApp} illustrates the Laplace
approximation to a 10-DOF $\chi^2$ distribution.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{img/week3_LaplaceApp.pdf}
    \caption{ Laplace approximation to a 10-DOF $\chi^2$ distribution.}
    \label{fig:LaplaceApp}
\end{figure}


\begin{example}[Laplace approximation for Bayesian logistic regression]
Consider a one-dimensional Bayesian logistic regression model.
Let $\{(x_n,y_n)\}_{n=1}^N$ be observed data with $x_n\in\R$ and $y_n\in\{0,1\}$. The model is defined as
\[
w \sim \mathcal N(0,\tau^2),
\qquad
p(y_n=1 \mid w,x_n) = \sigma(wx_n),
\]
where $\sigma(t) = (1+e^{-t})^{-1}$ denotes the logistic sigmoid function, and latent variable is the regression weight $w\in\R$.

The posterior distribution is (using Bayes theorem)
\[
p(w\mid x,y)
=
\frac{p(y\mid w,x)p(w)}{p(y)}
\propto
\exp\!\left(-\frac{w^2}{2\tau^2}\right)
\prod_{n=1}^N
\sigma(wx_n)^{y_n}\bigl(1-\sigma(wx_n)\bigr)^{1-y_n}.
\]

Using $y\log\sigma(a)+(1-y)\log(1-\sigma(a)) = ya-\log(1+e^{a})$, we can write the log-posterior as
\[
\log p(w\mid x,y)
=
-\frac{w^2}{2\tau^2}
+\sum_{n=1}^N\Big(
y_n w x_n - \log(1+e^{w x_n})
\Big)
+\text{const}.
\]

Its first and second derivatives wrt $w$ are:
\begin{align}
  \nabla \log p(w\mid x,y)
  &=
  -\frac{w}{\tau^2}
  +\sum_{n=1}^N x_n\Big(y_n-\sigma(w x_n)\Big),\label{eq:ex_Bayeslogreg_J}
  \\[4pt]
  \nabla^2 \log p(w\mid x,y)
  &=
  -\frac{1}{\tau^2}
  -\sum_{n=1}^N x_n^2\,\sigma(w x_n)\bigl(1-\sigma(w x_n)\bigr).
  \label{eq:ex_Bayeslogreg_H}
\end{align}

Since the first-order optimality condition $\nabla \log p(w\mid x,y)=0$ from eq.~\eqref{eq:ex_Bayeslogreg_J} cannot be solved explicitly, the maximum can be obtained using the Newton method, that is, 
\[
w_\text{new} = w_\text{old} - \frac{\nabla \log p(w\mid x,y)}{\nabla^2 \log p(w\mid x,y)}\bigg|_{w=w_\text{old}}
\]

This approximation matches the posterior locally around the MAP, but fails to
capture global properties such as skewness or heavy tails.




\end{example}



\section{Variational inference}

\paragraph{The evidence lower bound (ELBO).} An alternative to estimating the posterior $p(z \mid x)$, is to specify a family of densities over $z$ termed $\cQ$, where each $q(\cdot)\in\cQ$ is a candidate approximation to the posterior. Our goal will be then to find the best candidate within $\cQ$, according to an optimality criterion given by the KL divergence, that is, we will find
\begin{equation}
    q^*(z) = \argmin_{q\in\cQ} \KL{q(z)}{p(z \mid x)}.
    \label{eq:VI_problem}
\end{equation}
\begin{remark}
    The more general or comprehensive the family $\cQ$, the more difficult it is to find the optimal $q^*(z)$.
\end{remark}

Observe that solving eq.~\eqref{eq:VI_problem} is unfeasible, since it requires $p(z\mid x)$ or equivalently $p(x)$. However, notice that since
\[
\KL{q(z)}{p(z \mid x)}
=
\int q(z)\log q(z) \dz 
-
\int q(z)\log p(z,x) \dz 
+
\underbrace{\int q(z)\log p(x) \dz}_{\log p(x)},
\]
the dependence of the objective on $\log p(x)$ is via a constant which can be ignored when optimising wrt $q(z)$. Therefore, the optimisation problem in eq.~\eqref{eq:VI_problem} is equivalent to maximising the evidence lower bound (ELBO) given by
\begin{equation}
    \elbo \defeq \int q(z)\log p(z,x) \dz  - \int q(z)\log q(z) \dz.
    \label{eq:elbo_def1}
\end{equation}

Let us observe the following decomposition of the ELBO:
\begin{align}
    \elbo 
    &= \int q(z)\log p(z,x) \dz  - \int q(z)\log q(z) \dz\nonumber\\
    &= \int q(z)\log p(x \mid z) \dz + \int q(z)\log p(z) \dz  - \int q(z)\log q(z) \dz\nonumber\\
    &= \E{\log p(x \mid z)} - \KL{q(z)}{p(z)},
    \label{eq:elbo_def2}
\end{align}
where all the expectations in this section will be wrt $q(z)$, unless other wise stated. 

Maximising the ELBO in eq.~\eqref{eq:elbo_def2} is a balance between two terms. The first one seeks to assign the mass of $q$ to the values of $z$ that explain the observations $x$, while the second one ensures that $q(z)$ is close to the prior $p(z)$. 

\begin{remark}
    Maximising the ELBO recovers the usual likelihood / prior trade off. 
\end{remark}

To justify the name of this objective, let us see how it relates to the so called \emph{evidence} $\log p(x)$. Using the expression in eq.~\eqref{eq:elbo_def1}, we have
\begin{align}
    \log p(x) - \elbo
    &= 
    \int q(z) \log p(x) \dz - \int q(z)\log p(z,x) \dz  + \int q(z)\log q(z) \dz\nonumber\\
    &= 
    \int q(z) \log \frac{1}{p(z \mid x)} \dz  + \int q(z)\log q(z) \dz\nonumber\\
    &= \KL{q(z)}{p(z \mid x)}. \label{eq:elbo_gap}
\end{align}
Since the KL divergence is always nonegative, the ELBO is---as its name suggests---a lower bound of the evidence $\log p(x)$. That is, 
\[
\log p(x) \geq \elbo.
\]
Furthermore, the gap between these quantities is precisely the discrepancy, in terms of the KL, between the approximate posterior $q(z)$ and the true posterior. 

\begin{remark}[ELBO and parameter identification]
    When the latent variable is a parameter, the ELBO can be informally used for model selection, by finding the maximum a posteriori. However, when this is done in practice, it is unknown how far this solution is from the true one. 
    \missing{include an illustration of the biased maxima}
\end{remark}

\begin{remark}[Maximising the ELBO and EM]
    Observe from eq.~\eqref{eq:elbo_def1} that the first term in the ELBO is the objective of EM, this is because in EM we have that $\elbo = \log p(x)$, since $q(z) = p(z \mid x)$---see eq.~\eqref{eq:elbo_gap}. This is possible because EM is used in cases where $p(z \mid x)$ can be computed analytically. In VI, however, we do not assume tractable posteriors, but rather we only consider a sufficiently good approximation $q$ within the variational family $\cQ$. Therefore, VI can be seen as an extension of EM, used in cases where the posterior is intractable. 
\end{remark}

\paragraph{The mean field variational family.} An explicit family $\cQ$ can be chosen based on the trade-off between the family's expressivity and the feasibility of solving the problem in eq.~\eqref{eq:VI_problem}. We will consider the mean field (MF) family, where the distribution over the latent variable $z = (z_1,z_2,\ldots,z_m)\in\R^m$ factorises as\footnote{The name comes from mean-field theory in statistical physics: complex interactions are replaced by an average field acting on each variable. The fact that the optimal $q_i$ is in fact an average, will become clearer in the next section.}
\begin{equation}
    q(z) = \prod_{i=1}^mq_i(z_i).
\end{equation}
With the introduction of the MF family, the task of finding an $m$-dimensional distribution is replaced by finding $m$ 1-dimensional ones. 

\begin{remark}[Generality of the MF family]
The coordinate-wise densities are not specified in explicit form. Instead, appropriate expressions can be chosen for each coordinate according to its characteristics, for example whether the coordinate is continuous or discrete.
\end{remark}

\begin{example}[MF approximation for the hierarchical GMM]
    A choice of MF family for the hierarchical Gaussian mixture in Example \ref{ex:hierarchical_GMM} is
    \[
    q(\pi,\mu_{1:K}, z_{1:N}) = \mathrm{Dirichlet}(\pi; \alpha) \prod_{k=1}^{K} \cN(\mu_k; m_k, \Sigma_k) \prod_{n=1}^{N} \mathrm{Categorical}(z_n; \phi_n),
    \]
    where $\{\alpha, m_{1:K}, \Sigma_{1:K}, \phi_{1:N}\}$ are the variational parameters.
\end{example}

\begin{example}[MF underestimates the marginal variances]
    We will illustrate the ability of the MF family to capture marginals, but no dependencies between variables, and show how these captured marginals differ from the true ones. Let us consider $z = [z_1,z_2]^\top\in\R^2$, and the target 
    \[
    p(z \mid x)
    =
    \cN\!\left(
    z \,\middle|\,
    \begin{bmatrix}
    \mu_1 \\
    \mu_2
    \end{bmatrix},
    \begin{bmatrix}
    \sigma_{11} & \sigma_{12} \\
    \sigma_{21} & \sigma_{22}
    \end{bmatrix}
    \right).
    \]
    The optimal MF family for this target is\footnote{We will not prove this, as it requires results from variational calculus.}  $q(z) = q_1(z_1)q_2(z_2)$, where  
    \[
    q_1(z_1) = \cN(z_1 \mid m_1, s_1), \quad  q_2(z_2) = \cN(z_2 \mid m_2, s_2). 
    \]
    The  optimal variational parameters $\theta = \{m_1,s_1,m_2,s_2\}$ can be found by
    \[
    \theta^* = \argmin \KL{q_1(z_1)q_2(z_2)}{p(z_1,z_2 \mid x)},
    \]
    which admits a closed-form since the distributions are Gaussian. Expressing  \(q(z)=\mathcal N(z\mid m, S)\) with
    \[
    m =
    \begin{bmatrix}
    m_1\\
    m_2
    \end{bmatrix},
    \qquad
    S =
    \begin{bmatrix}
    s_1 & 0\\
    0 & s_2
    \end{bmatrix},
    \]
    we have
    \begin{align*}
    \KL{q}{p}
    &=
    \frac12\Big(
    \log\frac{\det\Sigma}{\det S}
    -2
    +\tr(\Sigma^{-1}S)
    +(m-\mu)^\top\Sigma^{-1}(m-\mu)
    \Big),
    \end{align*}
    where \(\mu=(\mu_1,\mu_2)^\top\) and \([\Sigma]_{ij}=\sigma_{ij}\).

    Taking derivatives with respect to \(m_1\) yields
    \[
    \frac{\partial}{\partial m_1}\KL{q}{p}
    =
    \big(\Sigma^{-1}(m-\mu)\big)_1,
    \]
    which vanishes if and only if \(m_1=\mu_1\). Next, differentiating with respect to \(s_1\) gives
    \[
    \frac{\partial}{\partial s_1}\KL{q}{p}
    =
    \frac12\Big(
    (\Sigma^{-1})_{11}
    -
    \frac{1}{s_1}
    \Big),
    \]
    so the optimum satisfies
    \[
    s_1 = \frac{1}{(\Sigma^{-1})_{11}}.
    \]
     By symmetry, we also have $m_2=\mu_2$ and $s_2 = \frac{1}{(\Sigma^{-1})_{22}}$.

    Therefore, the optimal mean-field approximation matches the marginal means but
    not the marginal variances:
    \[
    q_1(z_1)=\mathcal N\!\left(z_1\mid \mu_1,\;(\Sigma^{-1})_{11}^{-1}\right),
    \qquad
    q_2(z_2)=\mathcal N\!\left(z_2\mid \mu_2,\;(\Sigma^{-1})_{22}^{-1}\right).
    \]

    To conclude, let us see how the recovered variances $s_1$ and $s_2$ relate to the true ones. Since the precision matrix can be calculated explicitly in this case, we have 
    \[
    s_1 = \frac{1}{(\Sigma^{-1})_{11}} 
    = \left(\frac{\sigma_{22}}{\sigma_{11}\sigma_{22} - \sigma_{12}^2}\right)^{-1} = 
    \sigma_{11}\frac{\sigma_{22}\sigma_{11} - \sigma_{12}^2}{\sigma_{22}\sigma_{11}} = \sigma_{11}(1-\rho^2),
    \]
    where $\rho = \sigma_{12}/\sqrt{\sigma_{11}\sigma_{22}}$ is the correlation coefficient. Analogously, $s_2 =  \sigma_{22}(1-\rho^2)$. Therefore,
    \begin{itemize}
    \item When \(z_1\) and \(z_2\) are uncorrelated (\(\rho = 0\)), the mean-field
    variance matches the true marginal variance, that is \(s_1 = \sigma_{11}\).
    \item When \(|\rho| > 0\), the mean-field variance is strictly smaller than the marginal variance, revealing the ignored dependencies by the mean-field assumption.
    \item When \(|\rho| \to 1\), \(z_1\) and \(z_2\) are almost deterministically coupled, and we have \(s_1,s_2 \to 0\), meaning that case cannot be captured by a mean-field approximation.
    \end{itemize}

    As a consequence, this illustrates that mean-field variational inference
    systematically underestimates posterior uncertainty in the presence of strong
    correlations. Fig.~\ref{fig:MF_Gaussian} shows this phenomenon in the three cases indicated above. 

    \begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{img/week3_MF_Gaussian_0.pdf}
    \hfill
    \includegraphics[width=0.3\textwidth]{img/week3_MF_Gaussian_05.pdf}
    \hfill
    \includegraphics[width=0.3\textwidth]{img/week3_MF_Gaussian_95.pdf}
    \caption{Mean-field variational approximation of a 2D Gaussian: uncorrelated  and correlated cases.}
    \label{fig:MF_Gaussian}
\end{figure}
\end{example}


\paragraph{Coordiante ascent variational inference (CAVI).} 

Let us consider the \emph{complete conditional} given by 
\[
p(z_j \mid z_{-j},x),\]
where the subindex $\cdot_{-j}$ denotes all variables except the $j$-th one. Under the mean-field family, we cannot choose $q_j(z_j) = p(z_j \mid z_{-j},x)$ due to the independence assumption. However, let us see that the optimal choice for the MF approximation in the general case has a very similar form. 

Consider the update of a single coordinate of the latent variable $z_j$. Here, 
\begin{align}
    \elbo 
    &= \int q(z)\log p(x,z)\dz - \int q(z) \log q(z)\dz\nonumber\\
    &= \int q_j(z_j)
        \underbrace{\left(\int q_{-j}(z_{-j}) \log p(x,z_j, z_{-j})\dz_{-j}\right)}_{\mathbb{E}_{q_{-j}}\left[\log p(x,z_j, z_{-j})\right] }
       \dz_j - \int q_j(z_j)\log q_j(z_j)\dz_j + C(q_{-j}),\nonumber
\end{align}
where $ C(q_{-j})$ denotes terms that only depend on $q_{-j}$. Also, note that  the expectation in the argument admits the following proportional expression
\begin{equation}
    \mathbb{E}_{q_{-j}}\left[\log p(x,z_j, z_{-j})\right]  \propto_{q_j} \mathbb{E}_{q_{-j}}\left[\log p(z_j\mid x, z_{-j})\right], \label{eq:prop_expectations_CAVI}    
\end{equation}
where the symbol $\propto_{q_j} $ denotes proportionality wrt $q_j$ only. Therefore, we have
\begin{align}
    \elbo 
     &\propto_{q_j} 
    \int q_j(z_j) \left(\mathbb{E}_{q_{-j}}\left[\log p(z_j\mid x, z_{-j})\right]\right)  \dz_j -  \int q_j(z_j)\log q_j(z_j)\dz_j \nonumber\\
    &=
    - \int q_j(z_j)
    \log \frac
    {q_j(z_j)}
    {\exp \mathbb{E}_{q_{-j}}\left[\log p(z_j\mid x, z_{-j})\right]}
    \dz_j \nonumber\\
    &=\KL{q_j(z_j)}{\exp \mathbb{E}_{q_{-j}}\left[\log p(z_j\mid x, z_{-j})\right]} + \text{const}.\label{eq:CAVI}
\end{align}
Therefore, the optimal choice for $q_j(z_j)$ is $\exp \mathbb{E}_{q_{-j}}\left[\log p(z_j\mid x, z_{-j})\right]$. 

\begin{remark}[Mean-field interpretation]
    The mean-field family is named by analogy to mean-field models in statistical physics, where the effect that $j$-th particle receives from the remaining ones is expressed as the mean effect of the field. 
\end{remark}

\begin{remark}[]
    This optimal functional form for the MF family is not constrained to a particular expression for the density $q_j$ or for the model $p$. 
\end{remark}


\begin{remark}[CAVI update rule]
    Eq.~\eqref{eq:CAVI} provides an update rule for $q_j$ based on the remaining variational factors $q_{-j}$. Therefore, to compute the complete MF approximation, we first initialise all factors and implement Eq.~\eqref{eq:CAVI} iteratively for $j=1,\ldots,m$. Based on eq.~\eqref{eq:prop_expectations_CAVI}, this update rule can be implemented as 
    \begin{equation}
        \log q_j(z_j) \propto_{q_j} \mathbb{E}_{q_{-j}}\left[\log p(x,z)\right].
        \label{eq:CAVI_update}
    \end{equation}    
\end{remark}


\begin{example}[CAVI for a two-component Gaussian mixture]
Let $x_1,\dots,x_N\in\R$ be observations and let $z_n\in\{1,2\}$ be latent
assignments. Consider the model with fixed parameters $(\pi,\mu_1,\mu_2,\sigma^2)$:
\[
p(z_n=k)=\pi_k,
\qquad
p(x_n\mid z_n=k)=\mathcal N(x_n\mid \mu_k,\sigma^2),
\quad k\in\{1,2\}.
\]
We approximate the posterior with a mean-field family
\[
q(z_{1:N})=\prod_{n=1}^N q_n(z_n),
\qquad
q_n(z_n=k)=\phi_{nk},\ \ \sum_{k=1}^2\phi_{nk}=1.
\]
From eq.~\eqref{eq:CAVI_update}, CAVI updates each factor by
\[
\log q_n^*(z_n)=\mathbb{E}_{q_{-n}}[\log p(x_{1:N},z_{1:N})]+\text{const}.
\]
Since $z_n$ only appears in $\log p(z_n)+\log p(x_n\mid z_n)$, we obtain
\[
\phi_{nk}
\propto
\exp\!\Big(\log \pi_k + \log \mathcal N(x_n\mid \mu_k,\sigma^2)\Big)
=
\pi_k\,\mathcal N(x_n\mid \mu_k,\sigma^2),
\]
hence
\[
\phi_{nk}
=
\frac{\pi_k\,\mathcal N(x_n\mid \mu_k,\sigma^2)}
{\sum_{j=1}^2 \pi_j\,\mathcal N(x_n\mid \mu_j,\sigma^2)}.
\]
Thus, CAVI recovers the usual mixture responsibilities as a variational posterior.
\end{example}



\paragraph{Black-box variational inference (BBVI).}
To derive closed-form updates, CAVI relies on model-specific conjugacy thay may not hold in practice. This issue can be addressed with black-box variational
inference (BBVI), a Monte Carlo approach to maximise the ELBO that  treats the probabilistic model as a black box and thus is free from analytic updates.

Let $q_\lambda(z)$ denote a variational distribution parameterised by $\lambda$. As a function of the variational parameters, the
ELBO can be written as
\[
\elbo(\lambda)
=
\mathbb{E}_{q_\lambda(z)}\!\left[\log p(x,z)-\log q_\lambda(z)\right].
\]
Under mild regularity conditions, the gradient of the ELBO can be expressed as
\begin{align}
\nabla_\lambda \elbo(\lambda)
&=
\nabla_\lambda \int q_\lambda(z)\,\big(\log p(x,z)-\log q_\lambda(z)\big)\,dz
\nonumber\\
&=
\int \nabla_\lambda q_\lambda(z)\,\big(\log p(x,z)-\log q_\lambda(z)\big)\,dz
\qquad (\text{since} \int q_\lambda(z)\nabla_\lambda \log q_\lambda(z)\,dz = 0)
\nonumber\\
&=
\int q_\lambda(z)\,\big(\log p(x,z)-\log q_\lambda(z)\big)\,\nabla_\lambda \log q_\lambda(z)\,dz
\nonumber\\
&=
\mathbb{E}_{q_\lambda(z)}\!\left[
\big(\log p(x,z)-\log q_\lambda(z)\big)\,\nabla_\lambda \log q_\lambda(z)
\right].
\end{align}
This expression holds for any choice of variational family and any probabilistic model
for which $\log p(x,z)$ can be evaluated pointwise.

In practice, the expectation above can be approximated using Monte Carlo samples
$z^{(s)}\sim q_\lambda(z)$, yielding an unbiased estimator of the (stochastic) gradient. Therefore, the variational parameters $\lambda$ can be optimised via, e.g., stochastic gradient ascent. One caveat of this estimator is that---yet generally applicable---it suffers from high variance and thus slow convergence.

With the appropriate variance reductions techniques, BBVI is applicable to complex, non-conjugate models, with attractive scalability properties. This provides a foundation to combine probabilistic modelling with automatic differentiation at scale, that is appealing for the constructions of modern probabilistic generative models.


\section{Latent Dirichlet Allocation}

Latent Dirichlet Allocation (LDA) is a probabilistic generative model for collections of discrete data, in particular, for text corpora. This model serves as an illustration for the practical application of variational inference of both global and local latent variables.

Consider a corpus of $D$ documents, where document $d$ is a \emph{bag of words} comprising $w_{d1},\dots,w_{dN_d}$ words drawn from a vocabulary of size $V$. Additionally, assume $K$ latent topics, each represented by a distribution over words.

\begin{figure}[t]
\centering
\begin{tikzpicture}

\node[latent] (theta) {$\theta_d$};
\node[latent, below=1.4cm of theta] (z) {$z_{dn}$};
\node[obs, right=2.6cm of z] (w) {$w_{dn}$};

% beta must come AFTER w if you place it "right of w"
\node[latent, right=2.6cm of w] (beta) {$\beta_k$};

\edge {theta} {z};
\edge {z} {w};
\edge {beta} {w};

\plate {plateN} {(z)(w)} {$n=1,\dots,N_d$};
\plate {plateD} {(theta)(plateN)} {$d=1,\dots,D$};
\plate {plateK} {(beta)} {$k=1,\dots,K$};

\end{tikzpicture}
\caption{Graphical model of Latent Dirichlet Allocation (LDA).
Global topic--word distributions $\beta_k$ generate observed words $w_{dn}$ through
latent topic assignments $z_{dn}$, while document-specific topic proportions $\theta_d$
control topic usage within each document.}
\label{fig:lda_graphical_model}
\end{figure}


The generative model is presented in Fig.~\ref{fig:lda_graphical_model} and described as follows: 
\begin{itemize}
    \item For each topic $k=1,\dots,K$, 
    \[
    \beta_k \sim \mathrm{Dirichlet}(\eta),
    \]
    where $\beta_k \in \Delta^{V-1}$, is the topic--word distribution.
    \item For each document $d=1,\dots,D$:
    \begin{itemize}
        \item Draw topic proportions
        \[
        \theta_d \sim \mathrm{Dirichlet}(\alpha).
        \]
        \item For each word $n=1,\dots,N_d$, draw its topic assignment and then the word:
        \[
        z_{dn} \sim \mathrm{Categorical}(\theta_d),
        \qquad
        w_{dn} \sim \mathrm{Categorical}(\beta_{z_{dn}}).
        \]
    \end{itemize}
\end{itemize}

\begin{remark}[Latent and observed variables in the model]
    In the LDA model, the topic--word distributions $\beta_k$ are the global latent variables, the topic proportions $\theta_d$ are document-level local latent variables, and the assignments $z_{dn}$ are word-level local latent variables. Lastly, the words $w_{dn}$ are the observed variables
\end{remark}

The joint distribution factorises as
\[
p(w,z,\theta,\beta)
=
\prod_{k=1}^K p(\beta_k)
\prod_{d=1}^D p(\theta_d)
\prod_{n=1}^{N_d} p(z_{dn}\mid \theta_d)\,p(w_{dn}\mid z_{dn},\beta).
\]

Given the observed words $w$, the posterior
$p(z,\theta,\beta \mid w)$
is analytically intractable due to the coupling between topic assignments, topic
proportions, and topic--word distributions. Therefore, the posterior can be approximated using a mean-field family:
\[
q(\beta,\theta,z)
=
\prod_{k=1}^K q(\beta_k)
\prod_{d=1}^D q(\theta_d)
\prod_{d=1}^D \prod_{n=1}^{N_d} q(z_{dn}),
\]
where
\[
q(\beta_k)=\mathrm{Dirichlet}(\lambda_k),
\quad
q(\theta_d)=\mathrm{Dirichlet}(\gamma_d),
\quad
q(z_{dn})=\mathrm{Categorical}(\phi_{dn}).
\]

Therefore, using CAVI, each factor is updated by taking the
expected log-joint distribution with respect to the remaining variables. 

\paragraph{Word-level update.} The topic assignment $z_{dn}$ is given by
\begin{align}
\log q^*(z_{dn}=k)
&=
\mathbb{E}_{q(\theta_d,\beta)}
\!\left[
\log p(z_{dn}=k,w_{dn}\mid \theta_d,\beta)
\right]
+ \text{const}
\\
&=
\mathbb{E}_{q(\theta_d)}[\log \theta_{dk}]
+
\mathbb{E}_{q(\beta_k)}[\log \beta_{k,w_{dn}}]
+ \text{const}
\\
\Rightarrow\quad
\phi_{dnk}
&\propto
\exp\!\Big(
\mathbb{E}_{q(\theta_d)}[\log \theta_{dk}]
+
\mathbb{E}_{q(\beta_k)}[\log \beta_{k,w_{dn}}]
\Big),
\qquad k=1,\dots,K .
\end{align}
with normalisation $\sum_{k=1}^K \phi_{dnk}=1$. Using properties of the Dirichlet
distribution, these expectations can be calculated in closed form.

\paragraph{Document-level update.} The topic proportions are given by 
\[
\gamma_{dk}
=
\alpha_k + \sum_{n=1}^{N_d} \phi_{dnk},
\qquad k=1,\dots,K,
\]
which corresponds to adding the expected topic counts in document $d$ to the prior
parameter $\alpha$.

\paragraph{Global update.} Finally, the topic--word distributions is
\[
\lambda_{kv}
=
\eta_v + \sum_{d=1}^D \sum_{n=1}^{N_d} \phi_{dnk}\,\mathbf 1\{w_{dn}=v\},
\qquad k=1,\dots,K,\; v=1,\dots,V,
\]
that is, the prior parameter $\eta$ plus the expected number of times word $v$ is
assigned to topic $k$ across the entire corpus.

LDA models each document (in a group of documents) as a mixture of topics, and VI estimates the (soft) topic assignments for each word, as well as  document-level topic mixtures, and global topic--word distributions. 

